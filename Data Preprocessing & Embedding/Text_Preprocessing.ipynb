{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c9adb9-9296-42bb-8297-faf988ca810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b06c95-1be4-49e9-a716-ec59a98a9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"IMDB Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afae383-6eef-44de-8083-89c9767ee861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3243ea9c-f4d4-47e8-bfa5-053b7d100b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6632afdc-95ff-4189-b278-e6643b066e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930dbde8-25aa-4e36-9d6a-85f82725a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP FOR TEXT PROCESSING\n",
    "### LOWERCASE\n",
    "### REMOVE HTML\n",
    "### REMOVE URL\n",
    "### PUNCTUATION HANDLING\n",
    "### CHAT CONVRSION HANDLE\n",
    "### INCORRECT HANDLING\n",
    "###  STOP WORD\n",
    "### REMOVE EMOJI HANDLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87cf24d-dcc9-4e0b-9186-d9f4f90f0eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  LOWERCASE\n",
    "\n",
    "df['review'][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3e3b34-61f8-49de-98b0-d303920c1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23dcbcfa-2749-494e-b935-b6285dbc1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9caab49a-7e43-4554-9bdc-27a970619cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb8d4bf-1e34-4c60-9269-aefd3b35a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c6aaa28-20ab-4da3-81cd-d3654e9f9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22fb74b-0f69-4602-99ab-f59d72c92b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "060704d9-e4fa-4ac1-8fc1-28be59530fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5de185c5-d063-4c7b-8adc-3a600afd78fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times in the last 25 years. paul lukas\\' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight. the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. and the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. if i had a dozen thumbs, they\\'d all be \"up\" for this movie.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90d93ac-207f-47a5-8f89-36aa0a08fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d49b7060-78ab-4ab9-a17f-9c104d8d715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\./s+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a901bcb4-a532-4806-9ecc-deaaff423bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text1 = 'Check out my youtube https://www.youtube.com/dswithbappy dswithbappy'\n",
    "text2 = 'Check out my linkedin https://www.linkedin.com/in/boktiarahmed73/'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For data click https://www.kaggle.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4069449-0b20-4af5-96a7-8b6205225542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my linkedin '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae132e8-beff-4261-b452-a06ceaa0ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c2256c-04b0-44c8-ac9a-8ef887be2865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef3b3762-a76e-479a-9171-6ea01a8d638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude=string.punctuation\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d24ef84-123b-4bf5-a060-3a8d8e50bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba43e9b6-bc3c-47e1-96e8-7d3c8083902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='string. With. Punctuation?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49272e4e-5574-4baf-9a7c-89f0e2472c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With Punctuation\n",
      "66.70951843261719\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc(text))\n",
    "time1=time.time() -start\n",
    "print(time1*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5e236-f586-4d9c-8678-edce02f9b127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35fb8c79-a584-41f0-b528-c9785315f91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<!-- Explanation\\nstart = time.time():\\n\\nThis records the current time in seconds since the epoch (January 1, 1970, 00:00:00 UTC) as a floating point number. It's used as the starting timestamp.\\n\\nprint(remove_punc(text)):\\n\\nThis prints the result of the function remove_punc(text). Presumably, this function removes punctuation from the string text.\\n\\ntime1 = time.time() - start:\\n\\nAfter removing punctuation and printing the result, this line calculates the elapsed time by subtracting the start timestamp from the current time. The result, time1, is the time taken to execute remove_punc(text) and the print statement, measured in seconds.\\n\\nprint(time1 * 50000):\\n\\nThis prints the value of time1 multiplied by 50,000.\\n\\nFor example, if time1 was 0.05 seconds, time1 * 50000 would be 2,500.\\n\\nWhy Multiply by 50,000?\\nMultiplying by 50,000 essentially scales up the elapsed time. This could be done to:\\n\\nConvert the result into another unit or scale that's more readable (e.g., if you want to estimate how long this task would take for 50,000 iterations).\\n\\nVisually amplify small time measurements so they're easier to see or compare.\\n\\nSummary Table\\nCode Line\\tPurpose\\nstart = time.time()\\tRecord starting time (in seconds since epoch)\\nprint(remove_punc(...))\\tPrint result of punctuation removal\\ntime1 = time.time()-start\\tCompute elapsed time for above steps (in seconds)\\nprint(time1*50000)\\tPrint elapsed time scaled by 50,000 (e.g., to estimate bulk operation)\\nIn short, this snippet measures how long it takes to remove punctuation from a text and then estimates how much time 50,000 such operations would take.\\n\\n -->\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''<!-- Explanation\n",
    "start = time.time():\n",
    "\n",
    "This records the current time in seconds since the epoch (January 1, 1970, 00:00:00 UTC) as a floating point number. It's used as the starting timestamp.\n",
    "\n",
    "print(remove_punc(text)):\n",
    "\n",
    "This prints the result of the function remove_punc(text). Presumably, this function removes punctuation from the string text.\n",
    "\n",
    "time1 = time.time() - start:\n",
    "\n",
    "After removing punctuation and printing the result, this line calculates the elapsed time by subtracting the start timestamp from the current time. The result, time1, is the time taken to execute remove_punc(text) and the print statement, measured in seconds.\n",
    "\n",
    "print(time1 * 50000):\n",
    "\n",
    "This prints the value of time1 multiplied by 50,000.\n",
    "\n",
    "For example, if time1 was 0.05 seconds, time1 * 50000 would be 2,500.\n",
    "\n",
    "Why Multiply by 50,000?\n",
    "Multiplying by 50,000 essentially scales up the elapsed time. This could be done to:\n",
    "\n",
    "Convert the result into another unit or scale that's more readable (e.g., if you want to estimate how long this task would take for 50,000 iterations).\n",
    "\n",
    "Visually amplify small time measurements so they're easier to see or compare.\n",
    "\n",
    "Summary Table\n",
    "Code Line\tPurpose\n",
    "start = time.time()\tRecord starting time (in seconds since epoch)\n",
    "print(remove_punc(...))\tPrint result of punctuation removal\n",
    "time1 = time.time()-start\tCompute elapsed time for above steps (in seconds)\n",
    "print(time1*50000)\tPrint elapsed time scaled by 50,000 (e.g., to estimate bulk operation)\n",
    "In short, this snippet measures how long it takes to remove punctuation from a text and then estimates how much time 50,000 such operations would take.\n",
    "\n",
    " -->'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f130fc31-4c8a-40b4-9cba-5161b1ce3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "900dcce6-3543-4169-aabd-04825ec5e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "remove_punc1(text)\n",
    "time2=time.time()-start\n",
    "print(time2*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcafbcad-180d-494d-97a3-3e534c77bf33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtime1\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtime2\u001b[49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "time1/time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c357835-1b3b-4ae8-8580-f9e67c9fd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bdb2d-d157-4f37-bf15-bc2dcd2678c9",
   "metadata": {},
   "source": [
    "Aapke code me time.time() ka use isliye hai, taaki aap yeh dekh saken ki koi function ya code block kitne samay me run hota hai.\n",
    "\n",
    "Kaise kaam karta hai?\n",
    "start = time.time()\n",
    "\n",
    "Jab code run hota hai, tab ka exact samay store kar leta hai (epoch se ab tak ke seconds).\n",
    "\n",
    "[Aapka function run hota hai]\n",
    "\n",
    "Jaise: print(remove_punc(text))\n",
    "\n",
    "time1 = time.time() - start\n",
    "\n",
    "Jab function khatam hota hai, firse current samay leta hai.\n",
    "\n",
    "Dono samayon ka difference nikalta hai = function ko run karne me kitna waqt laga (seconds me)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1902eb-ec4e-4a99-8b16-1be43ff56ced",
   "metadata": {},
   "source": [
    "# Chat_conversion Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f2ceb71-8e90-43bf-91c8-09fd71abc406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FYI': 'For Your Information',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BTW': 'By The Way',\n",
       " 'OMG': 'Oh My God',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'LOL': 'Laugh Out Loud',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'GTG': 'Got To Go',\n",
       " 'TTYT': 'Talk To You Tomorrow',\n",
       " 'IDK': \"I Don't Know\",\n",
       " 'TMI': 'Too Much Information',\n",
       " 'IMHO': 'In My Humble Opinion',\n",
       " 'ICYMI': 'In Case You Missed It',\n",
       " 'AFAIK': 'As Far As I Know',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'TGIF': \"Thank God It's Friday\",\n",
       " 'FYA': 'For Your Action'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = {\n",
    "    'AFAIK':'As Far As I Know',\n",
    "    'AFK':'Away From Keyboard',\n",
    "    'ASAP':'As Soon As Possible'\n",
    "}\n",
    "\n",
    "\n",
    "{\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"OMG\": \"Oh My God\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"LOL\": \"Laugh Out Loud\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"GTG\": \"Got To Go\",\n",
    "    \"TTYT\": \"Talk To You Tomorrow\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"TMI\": \"Too Much Information\",\n",
    "    \"IMHO\": \"In My Humble Opinion\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"TGIF\": \"Thank God It's Friday\",\n",
    "    \"FYA\": \"For Your Action\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26b2663f-f4c0-4940-9818-dab40f9ec153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "843fbd78-b3fd-48ee-b03a-3731a52c0085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do this work As Soon As Possible'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('Do this work ASAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eed325-9bff-4c11-a8ba-c9f74cef3100",
   "metadata": {},
   "source": [
    "# Incorrect_text Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c21d3382-c2c5-4438-9dba-727b4c2ca01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3f19a2c-dd49-44fb-9386-5d5677e3d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aff824a5-4811-41b2-a0d4-f4292fa92f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58e34b83-4c06-49ed-9baa-6340bd620621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c7c49-2db7-4c91-b0d8-11567e298bf1",
   "metadata": {},
   "source": [
    "# STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1636285b-e2c4-4e30-9aa7-70e52f8eeb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5205e8cd-daea-481a-8c18-ac257c36b992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0b7a435-d972-4974-84a6-c7d730e15363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99366faf-0986-45f8-815d-cb91bf5f5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b17fdc48-163b-42eb-8afa-14bde98f63f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0296fad0-72f9-4e43-a2c5-dac43065c45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdd37702-ca86-4bd2-8069-dce2c280d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one    reviewers  mentioned   watching  1 oz e...\n",
       "1         wonderful little production.  filming techniq...\n",
       "2         thought    wonderful way  spend time    hot s...\n",
       "3        basically there's  family   little boy (jake) ...\n",
       "4        petter mattei's \"love   time  money\"   visuall...\n",
       "                               ...                        \n",
       "49995     thought  movie    right good job.    creative...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997       catholic taught  parochial elementary schoo...\n",
       "49998     going    disagree   previous comment  side  m...\n",
       "49999     one expects  star trek movies   high art,   f...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0624c-1bfb-45b9-a78b-2f89b4035589",
   "metadata": {},
   "source": [
    "# remove_emoji handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ee468cd-8b8c-4b60-9344-a3b86d1fcee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb4c0231-1da8-4cbd-9250-e5e34ecf1aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie it was '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie it was 😘😘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce357801-726e-4fc1-9095-6c5bfd2ab514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61f02b75-1ce3-4ae8-b4c5-8055d6a80a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is 🔥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5634f957-41f6-4426-8347-bf5c1e966ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Loved the movie. It was 😘'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af9072-f740-4792-ac53-3ef7e1a7d8f8",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580b022-e064-4e78-b2d8-f8a887ad76f5",
   "metadata": {},
   "source": [
    "## 1,Using the split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a5cf085-0784-474c-bcde-c62e7df9b9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenisation\n",
    "sent1='I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fb42769-45a2-4b2c-a301-332e6df2cbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " 'I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2='I am going to delhi.I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1cae212-e93e-4346-a7fe-79363f3ec031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem with split function\n",
    "sent3='I am going to delhi!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa9f0a32-4629-4f44-9791-30270c75d998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaab7d9-ba02-4970-87a0-8bd943a7edfc",
   "metadata": {},
   "source": [
    "# 2. Regular Expresseion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92176ef1-560d-49b9-8ca3-22d43dfe816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3='I am going to delhi!'\n",
    "tokens=re.findall(\"[\\w']+\",sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3b8d241-bf38-4d17-9ca0-fd958019dbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353818ba-53e5-4d9a-829f-ead3b61ab928",
   "metadata": {},
   "source": [
    "# 3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70faf06f-a4a7-4895-a00f-0207cb96f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4e9bd38-de02-45fe-9e15-a417cc6d9c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1='I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996f608-e9d6-498a-ad0e-e73bbc759bb0",
   "metadata": {},
   "source": [
    "Why this happens\n",
    "\n",
    "Old code/tutorials only told us to use nltk.download('punkt').\n",
    "\n",
    "But after NLTK v3.8, they separated language-specific data into punkt_tab.\n",
    "\n",
    "So your sent_tokenize was looking for English rules inside punkt_tab/english, but couldn’t find them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6754f-fd89-4dcb-8076-b72589187117",
   "metadata": {},
   "source": [
    "<!-- # from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Ye NLTK (Natural Language Toolkit) library ka part hai jo text ko tokens me todne ke liye use hota hai.\n",
    "\n",
    "# word_tokenize sentence ko shabdon (words) me todta hai.\n",
    "\n",
    "# sent_tokenize text ko sentences me todta hai.\n",
    "\n",
    "# nltk.download('punk')\n",
    "\n",
    "# Yahan typographical mistake hai. Sahi naam hai 'punkt' (not 'punk').\n",
    "\n",
    "# Punkt ek pre-trained tokenizer model hai jo NLTK me sentences ko correctly identify karne ke liye use hota hai.\n",
    "\n",
    "# Iska kaam hai sentences ke boundaries (ends and beginnings) ko identify karna, jaise ki \"Mr. Smith\" me full stop ko sentence end na samjhe.\n",
    "\n",
    "# Iska use kyu hota hai?\n",
    "# Natural Language Processing me jab aapko text ko sentences aur words me todna hota hai tab Punkt tokenizer ka use hota hai.\n",
    "\n",
    "# Ye model sentences ko accurately split karta hai, punctuation aur abbreviations ko samajh ke.\n",
    "\n",
    "# Sahi code: -->"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45dea942-835a-46c3-b15a-1ca6c2392c5f",
   "metadata": {},
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "Ye NLTK (Natural Language Toolkit) library ka part hai jo text ko tokens me todne ke liye use hota hai.\n",
    "\n",
    "word_tokenize sentence ko shabdon (words) me todta hai.\n",
    "\n",
    "sent_tokenize text ko sentences me todta hai.\n",
    "\n",
    "nltk.download('punk')\n",
    "\n",
    "Yahan typographical mistake hai. Sahi naam hai 'punkt' (not 'punk').\n",
    "\n",
    "Punkt ek pre-trained tokenizer model hai jo NLTK me sentences ko correctly identify karne ke liye use hota hai.\n",
    "\n",
    "Iska kaam hai sentences ke boundaries (ends and beginnings) ko identify karna, jaise ki \"Mr. Smith\" me full stop ko sentence end na samjhe.\n",
    "\n",
    "Iska use kyu hota hai?\n",
    "Natural Language Processing me jab aapko text ko sentences aur words me todna hota hai tab Punkt tokenizer ka use hota hai.\n",
    "\n",
    "Ye model sentences ko accurately split karta hai, punctuation aur abbreviations ko samajh ke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be7a1e2e-25ff-46ed-98cc-5e5b5a3f717c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98647aed-f557-4085-8e96-f274395ed61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29108c27-3c35-4f00-9f7b-a2d7d0ae4112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ec9ae-4aaa-491f-ac21-9d1f068bff6c",
   "metadata": {},
   "source": [
    "# 4.Spacy (Good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8b03978-2ad0-4119-ad91-b00b8bdbc612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (76.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.3-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.8.7-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.9 MB 429.7 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.5/14.9 MB 429.7 kB/s eta 0:00:34\n",
      "   -- ------------------------------------- 0.8/14.9 MB 516.0 kB/s eta 0:00:28\n",
      "   -- ------------------------------------- 0.8/14.9 MB 516.0 kB/s eta 0:00:28\n",
      "   -- ------------------------------------- 1.0/14.9 MB 518.8 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.0/14.9 MB 518.8 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 1.3/14.9 MB 545.6 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 1.3/14.9 MB 545.6 kB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 1.6/14.9 MB 562.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.6/14.9 MB 562.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.6/14.9 MB 562.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 556.0 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 556.0 kB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 2.1/14.9 MB 553.9 kB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 2.1/14.9 MB 553.9 kB/s eta 0:00:24\n",
      "   ------ --------------------------------- 2.4/14.9 MB 563.9 kB/s eta 0:00:23\n",
      "   ------ --------------------------------- 2.4/14.9 MB 563.9 kB/s eta 0:00:23\n",
      "   ------- -------------------------------- 2.6/14.9 MB 572.0 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 2.6/14.9 MB 572.0 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 2.9/14.9 MB 578.4 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 2.9/14.9 MB 578.4 kB/s eta 0:00:21\n",
      "   -------- ------------------------------- 3.1/14.9 MB 585.9 kB/s eta 0:00:21\n",
      "   -------- ------------------------------- 3.1/14.9 MB 585.9 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 3.4/14.9 MB 592.1 kB/s eta 0:00:20\n",
      "   --------- ------------------------------ 3.7/14.9 MB 605.8 kB/s eta 0:00:19\n",
      "   --------- ------------------------------ 3.7/14.9 MB 605.8 kB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 3.9/14.9 MB 619.7 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 3.9/14.9 MB 619.7 kB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 4.2/14.9 MB 621.3 kB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 4.5/14.9 MB 633.1 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 4.7/14.9 MB 643.8 kB/s eta 0:00:16\n",
      "   ------------ --------------------------- 4.7/14.9 MB 643.8 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 5.0/14.9 MB 643.8 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 5.2/14.9 MB 661.3 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 5.2/14.9 MB 661.3 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 5.5/14.9 MB 669.7 kB/s eta 0:00:15\n",
      "   --------------- ------------------------ 5.8/14.9 MB 685.4 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 6.0/14.9 MB 700.3 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 6.0/14.9 MB 700.3 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 6.3/14.9 MB 697.8 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 6.8/14.9 MB 731.9 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 7.1/14.9 MB 740.5 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 7.3/14.9 MB 758.7 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 7.9/14.9 MB 796.3 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 8.4/14.9 MB 825.5 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 8.7/14.9 MB 837.5 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 9.2/14.9 MB 869.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 9.2/14.9 MB 869.5 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 9.7/14.9 MB 886.9 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 10.0/14.9 MB 903.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 10.7/14.9 MB 949.2 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 11.0/14.9 MB 956.6 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 11.5/14.9 MB 984.1 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 12.1/14.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 12.3/14.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 12.8/14.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 13.6/14.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 13.9/14.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.3/632.3 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp310-cp310-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.2 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.2/6.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.0/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.1/5.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.9/5.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading wrapt-1.17.3-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\n",
      "   --- ------------------------------------  2/24 [wasabi]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ----------- ----------------------------  7/24 [mdurl]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   ------------------ --------------------- 11/24 [blis]\n",
      "   -------------------- ------------------- 12/24 [srsly]\n",
      "   -------------------- ------------------- 12/24 [srsly]\n",
      "   -------------------- ------------------- 12/24 [srsly]\n",
      "   -------------------- ------------------- 12/24 [srsly]\n",
      "   -------------------- ------------------- 12/24 [srsly]\n",
      "   -------------------- ------------------- 12/24 [srsly]\n",
      "   --------------------- ------------------ 13/24 [smart-open]\n",
      "   ------------------------- -------------- 15/24 [markdown-it-py]\n",
      "   ------------------------- -------------- 15/24 [markdown-it-py]\n",
      "   ------------------------- -------------- 15/24 [markdown-it-py]\n",
      "   -------------------------- ------------- 16/24 [language-data]\n",
      "   -------------------------- ------------- 16/24 [language-data]\n",
      "   -------------------------- ------------- 16/24 [language-data]\n",
      "   -------------------------- ------------- 16/24 [language-data]\n",
      "   -------------------------- ------------- 16/24 [language-data]\n",
      "   ---------------------------- ----------- 17/24 [rich]\n",
      "   ---------------------------- ----------- 17/24 [rich]\n",
      "   ---------------------------- ----------- 17/24 [rich]\n",
      "   ---------------------------- ----------- 17/24 [rich]\n",
      "   ------------------------------ --------- 18/24 [langcodes]\n",
      "   ------------------------------- -------- 19/24 [confection]\n",
      "   --------------------------------- ------ 20/24 [typer]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ----------------------------------- ---- 21/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [weasel]\n",
      "   ------------------------------------ --- 22/24 [weasel]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   ---------------------------------------- 24/24 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 rich-14.1.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "854987a4-64b8-4d37-9c8e-08197ed7408f",
   "metadata": {},
   "source": [
    "spaCy kya hai?\n",
    "\n",
    "spaCy ek popular, free, aur open-source Python library hai jo Natural Language Processing (NLP) ke liye banayi gayi hai. Yeh large volumes of text ko high speed par process karne aur samajhne ke liye use hoti hai. Iska focus production-level applications par hai — matlab ye research ke alawa real world me text analytics, chatbots, document analysis jaise kaam ke liye design ki gayi hai.\n",
    "\n",
    "spaCy ke key features:\n",
    "Tokenization: Text ko words, punctuation marks, etc. me todna\n",
    "\n",
    "Part-of-speech tagging: Har word ko uske grammatical role (jaise noun, verb) ke roop me identify karna\n",
    "\n",
    "Dependency parsing: Words ke beech grammatical relationships samajhna\n",
    "\n",
    "Named Entity Recognition (NER): Text me se persons, organizations, locations jaise entities pehchanana\n",
    "\n",
    "Text Classification: Documents ya sentences ko categories me baantna\n",
    "\n",
    "Lemmatization: Words ke base forms tak lana (jaise \"running\" se \"run\")\n",
    "\n",
    "Support for multiple languages\n",
    "\n",
    "Efficient and fast processing\n",
    "\n",
    "Integration with machine learning frameworks jaise TensorFlow, PyTorch\n",
    "\n",
    "Kyon use karein?\n",
    "SpaCy bahut fast aur accurate hai, is wajah se industry me widely use hota hai.\n",
    "\n",
    "Ye NLP models ko train karne, fine-tune karne aur deploy karne me madad karta hai.\n",
    "\n",
    "Text data ko samajhne, analyze karne, aur automate karne ke liye bahut effective tools provide karta hai.\n",
    "\n",
    "Summary:\n",
    "SpaCy ek advanced NLP library hai jo Python me text processing aur artificial intelligence se judi applications banane ke liye use hoti hai. Yeh text ko samajhne aur process karne me madad karti hai production-grade solutions ke liye.\n",
    "\n",
    "Aap ise complex text analysis, chatbots, sentiment analysis, information extraction, aur machine learning projects me istemal kar sakte hain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9848f82-b719-41ee-9c3b-3b3e2fd84ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 911.5 kB/s eta 0:00:13\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 911.5 kB/s eta 0:00:13\n",
      "     ----- --------------------------------- 1.8/12.8 MB 838.9 kB/s eta 0:00:14\n",
      "     ----- --------------------------------- 1.8/12.8 MB 838.9 kB/s eta 0:00:14\n",
      "     ------ -------------------------------- 2.1/12.8 MB 762.4 kB/s eta 0:00:15\n",
      "     ------ -------------------------------- 2.1/12.8 MB 762.4 kB/s eta 0:00:15\n",
      "     ------ -------------------------------- 2.1/12.8 MB 762.4 kB/s eta 0:00:15\n",
      "     ------ -------------------------------- 2.1/12.8 MB 762.4 kB/s eta 0:00:15\n",
      "     ------- ------------------------------- 2.4/12.8 MB 651.5 kB/s eta 0:00:17\n",
      "     ------- ------------------------------- 2.4/12.8 MB 651.5 kB/s eta 0:00:17\n",
      "     ------- ------------------------------- 2.4/12.8 MB 651.5 kB/s eta 0:00:17\n",
      "     ------- ------------------------------- 2.4/12.8 MB 651.5 kB/s eta 0:00:17\n",
      "     ------- ------------------------------- 2.4/12.8 MB 651.5 kB/s eta 0:00:17\n",
      "     ------- ------------------------------- 2.6/12.8 MB 559.2 kB/s eta 0:00:19\n",
      "     ------- ------------------------------- 2.6/12.8 MB 559.2 kB/s eta 0:00:19\n",
      "     ------- ------------------------------- 2.6/12.8 MB 559.2 kB/s eta 0:00:19\n",
      "     ------- ------------------------------- 2.6/12.8 MB 559.2 kB/s eta 0:00:19\n",
      "     ------- ------------------------------- 2.6/12.8 MB 559.2 kB/s eta 0:00:19\n",
      "     -------- ------------------------------ 2.9/12.8 MB 506.9 kB/s eta 0:00:20\n",
      "     -------- ------------------------------ 2.9/12.8 MB 506.9 kB/s eta 0:00:20\n",
      "     -------- ------------------------------ 2.9/12.8 MB 506.9 kB/s eta 0:00:20\n",
      "     -------- ------------------------------ 2.9/12.8 MB 506.9 kB/s eta 0:00:20\n",
      "     -------- ------------------------------ 2.9/12.8 MB 506.9 kB/s eta 0:00:20\n",
      "     --------- ----------------------------- 3.1/12.8 MB 453.4 kB/s eta 0:00:22\n",
      "     --------- ----------------------------- 3.1/12.8 MB 453.4 kB/s eta 0:00:22\n",
      "     --------- ----------------------------- 3.1/12.8 MB 453.4 kB/s eta 0:00:22\n",
      "     --------- ----------------------------- 3.1/12.8 MB 453.4 kB/s eta 0:00:22\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 436.7 kB/s eta 0:00:22\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 436.7 kB/s eta 0:00:22\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 436.7 kB/s eta 0:00:22\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 436.7 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.7/12.8 MB 424.3 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.7/12.8 MB 424.3 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.7/12.8 MB 424.3 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.7/12.8 MB 424.3 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.9/12.8 MB 417.2 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.9/12.8 MB 417.2 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.9/12.8 MB 417.2 kB/s eta 0:00:22\n",
      "     ----------- --------------------------- 3.9/12.8 MB 417.2 kB/s eta 0:00:22\n",
      "     ------------ -------------------------- 4.2/12.8 MB 405.2 kB/s eta 0:00:22\n",
      "     ------------ -------------------------- 4.2/12.8 MB 405.2 kB/s eta 0:00:22\n",
      "     ------------ -------------------------- 4.2/12.8 MB 405.2 kB/s eta 0:00:22\n",
      "     ------------ -------------------------- 4.2/12.8 MB 405.2 kB/s eta 0:00:22\n",
      "     ------------- ------------------------- 4.5/12.8 MB 395.9 kB/s eta 0:00:22\n",
      "     ------------- ------------------------- 4.5/12.8 MB 395.9 kB/s eta 0:00:22\n",
      "     ------------- ------------------------- 4.5/12.8 MB 395.9 kB/s eta 0:00:22\n",
      "     ------------- ------------------------- 4.5/12.8 MB 395.9 kB/s eta 0:00:22\n",
      "     ------------- ------------------------- 4.5/12.8 MB 395.9 kB/s eta 0:00:22\n",
      "     -------------- ------------------------ 4.7/12.8 MB 386.4 kB/s eta 0:00:21\n",
      "     -------------- ------------------------ 4.7/12.8 MB 386.4 kB/s eta 0:00:21\n",
      "     -------------- ------------------------ 4.7/12.8 MB 386.4 kB/s eta 0:00:21\n",
      "     --------------- ----------------------- 5.0/12.8 MB 382.7 kB/s eta 0:00:21\n",
      "     --------------- ----------------------- 5.0/12.8 MB 382.7 kB/s eta 0:00:21\n",
      "     --------------- ----------------------- 5.0/12.8 MB 382.7 kB/s eta 0:00:21\n",
      "     --------------- ----------------------- 5.0/12.8 MB 382.7 kB/s eta 0:00:21\n",
      "     --------------- ----------------------- 5.2/12.8 MB 378.6 kB/s eta 0:00:20\n",
      "     --------------- ----------------------- 5.2/12.8 MB 378.6 kB/s eta 0:00:20\n",
      "     --------------- ----------------------- 5.2/12.8 MB 378.6 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 380.0 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 380.0 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 380.0 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 380.0 kB/s eta 0:00:20\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ----------------- --------------------- 5.8/12.8 MB 375.6 kB/s eta 0:00:19\n",
      "     ------------------ -------------------- 6.0/12.8 MB 356.9 kB/s eta 0:00:19\n",
      "     ------------------ -------------------- 6.0/12.8 MB 356.9 kB/s eta 0:00:19\n",
      "     ------------------ -------------------- 6.0/12.8 MB 356.9 kB/s eta 0:00:19\n",
      "     ------------------ -------------------- 6.0/12.8 MB 356.9 kB/s eta 0:00:19\n",
      "     ------------------ -------------------- 6.0/12.8 MB 356.9 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.3/12.8 MB 347.3 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.3/12.8 MB 347.3 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.3/12.8 MB 347.3 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.3/12.8 MB 347.3 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.6/12.8 MB 345.0 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.6/12.8 MB 345.0 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.6/12.8 MB 345.0 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.6/12.8 MB 345.0 kB/s eta 0:00:19\n",
      "     ------------------- ------------------- 6.6/12.8 MB 345.0 kB/s eta 0:00:19\n",
      "     -------------------- ------------------ 6.8/12.8 MB 341.8 kB/s eta 0:00:18\n",
      "     -------------------- ------------------ 6.8/12.8 MB 341.8 kB/s eta 0:00:18\n",
      "     -------------------- ------------------ 6.8/12.8 MB 341.8 kB/s eta 0:00:18\n",
      "     -------------------- ------------------ 6.8/12.8 MB 341.8 kB/s eta 0:00:18\n",
      "     --------------------- ----------------- 7.1/12.8 MB 341.6 kB/s eta 0:00:17\n",
      "     --------------------- ----------------- 7.1/12.8 MB 341.6 kB/s eta 0:00:17\n",
      "     --------------------- ----------------- 7.1/12.8 MB 341.6 kB/s eta 0:00:17\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 342.1 kB/s eta 0:00:16\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 342.1 kB/s eta 0:00:16\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 342.1 kB/s eta 0:00:16\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 342.1 kB/s eta 0:00:16\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 342.1 kB/s eta 0:00:16\n",
      "     ----------------------- --------------- 7.6/12.8 MB 337.5 kB/s eta 0:00:16\n",
      "     ----------------------- --------------- 7.6/12.8 MB 337.5 kB/s eta 0:00:16\n",
      "     ----------------------- --------------- 7.6/12.8 MB 337.5 kB/s eta 0:00:16\n",
      "     ----------------------- --------------- 7.6/12.8 MB 337.5 kB/s eta 0:00:16\n",
      "     ----------------------- --------------- 7.9/12.8 MB 336.9 kB/s eta 0:00:15\n",
      "     ----------------------- --------------- 7.9/12.8 MB 336.9 kB/s eta 0:00:15\n",
      "     ----------------------- --------------- 7.9/12.8 MB 336.9 kB/s eta 0:00:15\n",
      "     ----------------------- --------------- 7.9/12.8 MB 336.9 kB/s eta 0:00:15\n",
      "     ------------------------ -------------- 8.1/12.8 MB 336.9 kB/s eta 0:00:14\n",
      "     ------------------------ -------------- 8.1/12.8 MB 336.9 kB/s eta 0:00:14\n",
      "     ------------------------ -------------- 8.1/12.8 MB 336.9 kB/s eta 0:00:14\n",
      "     ------------------------ -------------- 8.1/12.8 MB 336.9 kB/s eta 0:00:14\n",
      "     ------------------------ -------------- 8.1/12.8 MB 336.9 kB/s eta 0:00:14\n",
      "     ------------------------ -------------- 8.1/12.8 MB 336.9 kB/s eta 0:00:14\n",
      "     ------------------------- ------------- 8.4/12.8 MB 330.8 kB/s eta 0:00:14\n",
      "     ------------------------- ------------- 8.4/12.8 MB 330.8 kB/s eta 0:00:14\n",
      "     ------------------------- ------------- 8.4/12.8 MB 330.8 kB/s eta 0:00:14\n",
      "     ------------------------- ------------- 8.4/12.8 MB 330.8 kB/s eta 0:00:14\n",
      "     -------------------------- ------------ 8.7/12.8 MB 327.9 kB/s eta 0:00:13\n",
      "     -------------------------- ------------ 8.7/12.8 MB 327.9 kB/s eta 0:00:13\n",
      "     -------------------------- ------------ 8.7/12.8 MB 327.9 kB/s eta 0:00:13\n",
      "     --------------------------- ----------- 8.9/12.8 MB 330.1 kB/s eta 0:00:12\n",
      "     --------------------------- ----------- 8.9/12.8 MB 330.1 kB/s eta 0:00:12\n",
      "     --------------------------- ----------- 8.9/12.8 MB 330.1 kB/s eta 0:00:12\n",
      "     --------------------------- ----------- 8.9/12.8 MB 330.1 kB/s eta 0:00:12\n",
      "     --------------------------- ----------- 9.2/12.8 MB 331.1 kB/s eta 0:00:11\n",
      "     --------------------------- ----------- 9.2/12.8 MB 331.1 kB/s eta 0:00:11\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 334.4 kB/s eta 0:00:11\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 334.4 kB/s eta 0:00:11\n",
      "     ----------------------------- --------- 9.7/12.8 MB 338.2 kB/s eta 0:00:10\n",
      "     ----------------------------- --------- 9.7/12.8 MB 338.2 kB/s eta 0:00:10\n",
      "     ----------------------------- --------- 9.7/12.8 MB 338.2 kB/s eta 0:00:10\n",
      "     ----------------------------- -------- 10.0/12.8 MB 341.1 kB/s eta 0:00:09\n",
      "     ----------------------------- -------- 10.0/12.8 MB 341.1 kB/s eta 0:00:09\n",
      "     ----------------------------- -------- 10.0/12.8 MB 341.1 kB/s eta 0:00:09\n",
      "     ----------------------------- -------- 10.0/12.8 MB 341.1 kB/s eta 0:00:09\n",
      "     ------------------------------ ------- 10.2/12.8 MB 341.5 kB/s eta 0:00:08\n",
      "     ------------------------------ ------- 10.2/12.8 MB 341.5 kB/s eta 0:00:08\n",
      "     ------------------------------ ------- 10.2/12.8 MB 341.5 kB/s eta 0:00:08\n",
      "     ------------------------------ ------- 10.2/12.8 MB 341.5 kB/s eta 0:00:08\n",
      "     ------------------------------- ------ 10.5/12.8 MB 332.0 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.5/12.8 MB 332.0 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.5/12.8 MB 332.0 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.5/12.8 MB 332.0 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.5/12.8 MB 332.0 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.7/12.8 MB 308.1 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.7/12.8 MB 308.1 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.7/12.8 MB 308.1 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.7/12.8 MB 308.1 kB/s eta 0:00:07\n",
      "     ------------------------------- ------ 10.7/12.8 MB 308.1 kB/s eta 0:00:07\n",
      "     -------------------------------- ----- 11.0/12.8 MB 298.7 kB/s eta 0:00:07\n",
      "     -------------------------------- ----- 11.0/12.8 MB 298.7 kB/s eta 0:00:07\n",
      "     -------------------------------- ----- 11.0/12.8 MB 298.7 kB/s eta 0:00:07\n",
      "     -------------------------------- ----- 11.0/12.8 MB 298.7 kB/s eta 0:00:07\n",
      "     -------------------------------- ----- 11.0/12.8 MB 298.7 kB/s eta 0:00:07\n",
      "     --------------------------------- ---- 11.3/12.8 MB 298.1 kB/s eta 0:00:06\n",
      "     --------------------------------- ---- 11.3/12.8 MB 298.1 kB/s eta 0:00:06\n",
      "     --------------------------------- ---- 11.3/12.8 MB 298.1 kB/s eta 0:00:06\n",
      "     ---------------------------------- --- 11.5/12.8 MB 299.6 kB/s eta 0:00:05\n",
      "     ---------------------------------- --- 11.5/12.8 MB 299.6 kB/s eta 0:00:05\n",
      "     ---------------------------------- --- 11.5/12.8 MB 299.6 kB/s eta 0:00:05\n",
      "     ---------------------------------- --- 11.5/12.8 MB 299.6 kB/s eta 0:00:05\n",
      "     ----------------------------------- -- 11.8/12.8 MB 299.9 kB/s eta 0:00:04\n",
      "     ----------------------------------- -- 11.8/12.8 MB 299.9 kB/s eta 0:00:04\n",
      "     ----------------------------------- -- 11.8/12.8 MB 299.9 kB/s eta 0:00:04\n",
      "     ----------------------------------- -- 11.8/12.8 MB 299.9 kB/s eta 0:00:04\n",
      "     ----------------------------------- -- 11.8/12.8 MB 299.9 kB/s eta 0:00:04\n",
      "     ----------------------------------- -- 12.1/12.8 MB 302.1 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 12.1/12.8 MB 302.1 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 12.1/12.8 MB 302.1 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 12.1/12.8 MB 302.1 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 12.1/12.8 MB 302.1 kB/s eta 0:00:03\n",
      "     ------------------------------------ - 12.3/12.8 MB 300.7 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 300.7 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 300.7 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 300.7 kB/s eta 0:00:02\n",
      "     -------------------------------------  12.6/12.8 MB 300.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 300.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 300.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 299.4 kB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb058814-642d-48af-832e-ff3d58ef181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1ce294e-49aa-4f7c-8f60-cef096af9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fecdc10f-a28a-439e-86fb-39a044567662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I am going to visit delhi!"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc4 = nlp(sent1)\n",
    "doc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a69d22f-b5aa-412f-a1d0-b2cc26354b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "visit\n",
      "delhi\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e508f87-5aca-4563-ad05-55a85771f374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad0f8e-a5ea-4420-ab78-db0bfc3f9fbc",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "863a29ce-ba2e-412f-8643-28a000c77e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5473fef-273f-4580-8989-9b570f624d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a10577-089b-44c6-8a43-ff279db30909",
   "metadata": {},
   "source": [
    "sample=\" walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bb083f7-5ce1-4d3c-b32b-bd06a2ee9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3bca1ae-ec84-4ec5-94c5-797da1809965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610b7e6-e0ce-47a5-be6b-07e356d52a4a",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c163f6-a081-4063-86eb-e784a99f29aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kushagra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "\n",
    "sentence_words=nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "     print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6ab9a-386a-4867-accb-08d54959cdb9",
   "metadata": {},
   "source": [
    "# NOTE: Stemming & lamatization are same to retrieve root words but lamatization is worked good. Lamatization is slow & stemming is fast"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d920e204-3e57-4ea1-984f-c37139cde24b",
   "metadata": {},
   "source": [
    "What does this code do?\n",
    "Imports and downloads:\n",
    "\n",
    "Imports necessary NLTK modules.\n",
    "\n",
    "Downloads 'wordnet' and 'omw-1.4' datasets, which provide word reference databases for lemmatization.\n",
    "\n",
    "Create a lemmatizer object:\n",
    "\n",
    "# WordNetLemmatizer() is an NLTK class used to get the base form (lemma) of a word.\n",
    "\n",
    "Input sentence and punctuation:\n",
    "\n",
    "Sentence is a string with multiple words and punctuation.\n",
    "\n",
    "punctuations string contains punctuation characters you want to remove.\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "nltk.word_tokenize(sentence) breaks the sentence into a list of words and punctuation marks (tokens).\n",
    "\n",
    "Removing punctuation:\n",
    "\n",
    "The loop removes punctuation tokens from the token list so only words remain.\n",
    "\n",
    "(Note: Removing items from a list while iterating can cause issues; better to use list comprehension.)\n",
    "\n",
    "Printing header:\n",
    "\n",
    "Prints a formatted table header with “Word” and “Lemma”.\n",
    "\n",
    "Lemmatization with POS:\n",
    "\n",
    "For each word in the token list, it prints the original word and its lemma.\n",
    "\n",
    "wordnet_lemmatizer.lemmatize(word, pos='v') finds the base form of the word assuming it is a verb (pos='v').\n",
    "\n",
    "For example, \"running\" will be lemmatized to \"run\", \"eating\" to \"eat\", etc.\n",
    "\n",
    "Why use lemmatization?\n",
    "Lemmatization reduces a word to its base or dictionary form (lemma).\n",
    "\n",
    "Unlike stemming, lemmas are real words.\n",
    "\n",
    "Helps in text analysis by grouping different forms of a word under one base form.\n",
    "\n",
    "Example output from your sentence:\n",
    "Word\tLemma\n",
    "He\tHe\n",
    "was\tbe\n",
    "running\trun\n",
    "and\tand\n",
    "eating\teat\n",
    "at\tat\n",
    "same\tsame\n",
    "time\ttime\n",
    "He\tHe\n",
    "has\thave\n",
    "bad\tbad\n",
    "habit\thabit\n",
    "of\tof\n",
    "swimming\tswim\n",
    "after\tafter\n",
    "playing\tplay\n",
    "long\tlong\n",
    "hours\thour\n",
    "in\tin\n",
    "the\tthe\n",
    "Sun\tSun\n",
    "Improvements note:\n",
    "For removing punctuations safely, use this instead of your loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f12c9-4302-4ddd-bd0e-9e073d71b5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
